# ç¬¬ 1 ç«  å®æˆ˜ 3ï¼šQwen 2.5 åŸç”Ÿ PyTorch æ¨ç†

### 1.2.5 å®æˆ˜ 3ï¼šQwen 2.5 åŸç”Ÿ PyTorch æ¨ç†

> ğŸ¯ **å®æˆ˜ç›®æ ‡**ï¼šæœ¬èŠ‚å±•ç¤ºå¦‚ä½•ä¸ä¾èµ– vLLM ç­‰æ¨ç†æ¡†æ¶ï¼Œç›´æ¥ä½¿ç”¨ **PyTorch + Transformers** åœ¨ AMD GPU ä¸Šè¿è¡Œ Qwen2.5 æ¨¡å‹æ¨ç†ã€‚
>
> ğŸ’¡ **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æ›´çµæ´»çš„æ§åˆ¶ã€ç ”ç©¶æ¨¡å‹å†…éƒ¨è¡Œä¸ºã€æˆ–åªéœ€ç®€å•å•å¡æ¨ç†çš„åœºæ™¯ã€‚

#### æ­¥éª¤ 1ï¼šç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
pip install torch transformers accelerate
```

#### æ­¥éª¤ 2ï¼šåˆ›å»ºæ¨ç†è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `qwen_pytorch_inference.py`ï¼š

```python
# file: qwen_pytorch_inference.py
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==========================================
# æ ¸å¿ƒé…ç½®åŒº
# ==========================================

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "./Qwen/Qwen2___5-7B-Instruct"

# è®¾å¤‡é€‰æ‹©
DEVICE = "cuda:0"

# ==========================================

def run_inference():
    print(f"=== AMD ROCm PyTorch æ¨ç†æµ‹è¯• ===")

    # æ‰“å°è®¾å¤‡ä¿¡æ¯
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        print(f"ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)} ({props.total_memory / 1024**3:.1f} GB)")
    else:
        print("[è­¦å‘Š] æœªæ£€æµ‹åˆ° ROCm/CUDA è®¾å¤‡ï¼Œå°†ä½¿ç”¨ CPU è¿è¡Œï¼ˆææ…¢ï¼‰")

    # åŠ è½½ Tokenizer
    print("\n[1/3] æ­£åœ¨åŠ è½½ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True,trust_remote_code=True)
    except Exception as e:
        print(f"[é”™è¯¯] Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    print("\n[2/3] æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡ (BFloat16)...")
    st = time.time()
    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,  # AMD MIç³»åˆ—/æ–°å¡æ¨è BF16
            device_map=DEVICE,
            trust_remote_code=True,
        )
    except Exception as e:
        print(f"[è‡´å‘½é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³ï¼Œè¯·å°è¯•ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚")
        return

    print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {time.time() - st:.2f} ç§’")

    # æ„å»ºå¯¹è¯
    prompt = "ä½ å¥½ï¼Œè¯·ç”¨è¿™å°é«˜æ€§èƒ½æ˜¾å¡ä¸ºæˆ‘å†™ä¸€é¦–å…³äº AMD æ˜¾å¡é€†è¢­çš„ä¸ƒè¨€ç»å¥ã€‚"
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ‰åæ¨ªæº¢çš„è¯—äººã€‚"},
        {"role": "user", "content": prompt}
    ]

    print("\n[3/3] å¼€å§‹æ¨ç†...")

    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # ç¼–ç è¾“å…¥
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    # ç”Ÿæˆæ–‡æœ¬
    st = time.time()
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    et = time.time()

    # è§£ç è¾“å‡º
    input_len = model_inputs.input_ids.shape[1]
    output_ids = generated_ids[:, input_len:]

    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    tokens_gen = output_ids.shape[1]
    speed = tokens_gen / (et - st)

    print("\n" + "="*20 + " ç”Ÿæˆç»“æœ " + "="*20)
    print(response)
    print("="*50)
    print(f"ç”Ÿæˆé€Ÿåº¦: {speed:.2f} tokens/s")
    print(f"æ˜¾å­˜å ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

if __name__ == "__main__":
    # å¯ç”¨å®éªŒæ€§ ROCm ä¼˜åŒ–
    import os
    os.environ["TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL"] = "1"
    run_inference()
```

#### æ­¥éª¤ 3ï¼šè¿è¡Œæ¨ç†

```bash
python qwen_pytorch_inference.py
```

#### âœ… é¢„æœŸè¾“å‡º

![alt text](images/Qwen2.5_torch.png)

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

| # | æè¿° | é“¾æ¥ |
| :--- | :--- | :--- |
| [1] | AMD ROCm 7.2 æ­£å¼å‘å¸ƒ:æ”¯æŒå¤šæ¬¾æ–°ç¡¬ä»¶,ä¼˜åŒ– Instinct AI æ€§èƒ½ | [é“¾æ¥](https://so.html5.qq.com/page/real/search_news?docid=70000021_7796976caaa35752) |
| [2] | AMD Expands AI Leadership Across Client, Graphics, and ... | [é“¾æ¥](https://www.amd.com/en/newsroom/press-releases/2026-1-5-amd-expands-ai-leadership-across-client-graphics-.html) |
| [3] | AI Acceleration with AMD Radeonâ„¢ Graphics Cards | [é“¾æ¥](https://www.amd.com/en/products/graphics/radeon-ai.html) |
| [4] | AMD ROCm 7.2 æ›´æ–°ç›¸å…³æŠ¥é“ï¼ˆITä¹‹å®¶ç­‰ç»¼åˆï¼‰ | [é“¾æ¥](https://so.html5.qq.com/page/real/search_news?docid=70000021_9816977467427752) |
| [5] | Day 0 Support for Qwen3-Coder-Next on AMD Instinct GPUs | [é“¾æ¥](https://www.amd.com/en/developer/resources/technical-articles/2026/day-0-support-for-qwen3-coder-next-on-amd-instinct-gpus.html) |
| [6] | ROCm 7 è½¯ä»¶ | [é“¾æ¥](https://www.amd.com/zh-cn/products/software/rocm/whats-new.html) |
| [7] | Ubuntu å°†åŸç”Ÿæ”¯æŒ AMD ROCm è½¯ä»¶ | [é“¾æ¥](https://so.html5.qq.com/page/real/search_news?docid=70000021_494693a705e92252) |
| [8] | Install PyTorch via PIP (Linux ROCm) | [é“¾æ¥](https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/install/installrad/native_linux/install-pytorch.html) |
| [9] | Install PyTorch via PIP (Windows ROCm) | [é“¾æ¥](https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/install/installrad/windows/install-pytorch.html) |
| [10] | ResNet for image classification using AMD GPUs | [é“¾æ¥](https://rocm.blogs.amd.com/artificial-intelligence/resnet/README.html) |

---


