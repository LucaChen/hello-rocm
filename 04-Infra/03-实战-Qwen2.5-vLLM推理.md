# ç¬¬ 1 ç«  å®æˆ˜ 2ï¼šQwen 2.5 æ¨¡å‹æ¨ç† Demoï¼ˆvLLM + ROCmï¼‰

### 1.2.4 å®æˆ˜ 2ï¼šQwen 2.5 æ¨¡å‹æ¨ç† Demoï¼ˆvLLM + ROCmï¼‰

> ğŸš€ **å®æˆ˜ç›®æ ‡**ï¼šæœ¬èŠ‚å±•ç¤ºå¦‚ä½•åœ¨ AMD GPU ä¸Šé€šè¿‡ **vLLM + ROCm 7** è¿è¡Œé˜¿é‡Œ Qwen2.5 ç³»åˆ—å¤§æ¨¡å‹çš„æ¨ç†ã€‚
>
> ğŸ’¡ **é€‚ç”¨æç¤º**ï¼šæœ¬ç¤ºä¾‹ä»¥ Qwen2.5-7B-Instruct ä¸ºä¾‹ï¼Œé€‚åˆæ¡Œé¢ Radeon å’Œæ•°æ®ä¸­å¿ƒ Instinct ç³»åˆ— GPUã€‚

#### æ­¥éª¤ 1ï¼šä½¿ç”¨ Docker å¯åŠ¨ vLLM ç¯å¢ƒ

ä½¿ç”¨ Docker å¯ä»¥å¿«é€Ÿè·å¾—ä¸€ä¸ªé¢„é…ç½®å¥½çš„ vLLM + ROCm ç¯å¢ƒï¼š

```bash
docker run -it \
  --network=host \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add=video \
  --ipc=host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --shm-size 8G \
  -v $(pwd):/workspace \
  --name vllm \
  rocm/vllm-dev:rocm7.2_navi_ubuntu24.04_py3.12_pytorch_2.9_vllm_0.14.0rc0
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | è¯´æ˜ |
| :--- | :--- |
| `--network=host` | ä½¿ç”¨ä¸»æœºç½‘ç»œï¼Œä¾¿äºè®¿é—®æœåŠ¡ |
| `--device=/dev/kfd --device=/dev/dri` | æŒ‚è½½ GPU è®¾å¤‡ |
| `--group-add=video` | æ·»åŠ åˆ° video ç»„ä»¥è®¿é—® GPU |
| `--ipc=host --shm-size 8G` | å…±äº«å†…å­˜é…ç½®ï¼Œç”¨äºå¤šè¿›ç¨‹é€šä¿¡ |
| `-v $(pwd):/workspace` | æŒ‚è½½å½“å‰ç›®å½•åˆ°å®¹å™¨çš„ /workspace |

#### æ­¥éª¤ 2ï¼šç¯å¢ƒå‡†å¤‡

è¿›å…¥å®¹å™¨åï¼Œå®‰è£…åŸºç¡€åº“ï¼š

```bash
pip install transformers accelerate
```

#### æ­¥éª¤ 3ï¼šä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ ModelScopeï¼‰

å®‰è£… ModelScopeï¼š

```bash
pip install modelscope
```

åœ¨ç»ˆç«¯è¾“å…¥ `python` è¿›å…¥äº¤äº’æ¨¡å¼ï¼š

```python
from modelscope import snapshot_download

# ä¸‹è½½åˆ°å½“å‰ç›®å½•
model_dir = snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='./')
print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_dir}")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```text
æ¨¡å‹å·²ä¸‹è½½åˆ°: ./Qwen/Qwen2___5-7B-Instructors
```

#### æ­¥éª¤ 4ï¼šå¯åŠ¨ vLLM æ¨ç†æœåŠ¡

```bash
python -m vllm.entrypoints.openai.api_server \
  --model ./Qwen/Qwen2___5-7B-Instruct \
  --host 0.0.0.0 \
  --port 3000 \
  --dtype float16 \
  --gpu-memory-utilization 0.9 \
  --swap-space 16 \
  --disable-log-requests \
  --tensor-parallel-size 1 \
  --max-num-seqs 64 \
  --max-num-batched-tokens 32768 \
  --max-model-len 32768 \
  --distributed-executor-backend mp
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | è¯´æ˜ |
| :--- | :--- |
| `--model` | æ¨¡å‹è·¯å¾„ |
| `--dtype float16` | ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•° |
| `--gpu-memory-utilization 0.9` | GPU æ˜¾å­˜åˆ©ç”¨ç‡ |
| `--swap-space 16` | Swap ç©ºé—´å¤§å°ï¼ˆGBï¼‰ |
| `--max-model-len 32768` | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ |

#### æ­¥éª¤ 5ï¼šæµ‹è¯•æ¨ç†æœåŠ¡

ä½¿ç”¨ curl å‘é€è¯·æ±‚ï¼š

```bash
curl -s http://127.0.0.1:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "./Qwen/Qwen2___5-7B-Instruct",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ Qwen2.5-7B-Instructã€‚"}
    ],
    "temperature": 0.7,
    "max_tokens": 256
  }' | jq .
```

#### âœ… é¢„æœŸç»“æœ

å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œä½ ä¼šæ”¶åˆ°ç±»ä¼¼ä»¥ä¸‹çš„ JSON å“åº”ï¼ŒåŒ…å« Qwen2.5 æ¨¡å‹ç”Ÿæˆçš„å›ç­”ï¼š

![alt text](images/Qwen2.5_vllm.png)

---


